{
  "nodes": [
    {
      "id": "node-0",
      "data": {
        "data": {},
        "num_inputs": 0,
        "num_outputs": 1,
        "operator": "ui",
        "hierarchy": [],
        "logs": "",
        "status": "finished",
        "outputs": [
          {
            "key": "1690675840436",
            "bucket": "olorencore-olorencorebucketf2d1649c-mrgace7zzhce",
            "reserved": "file"
          }
        ],
        "subcomponents": {},
        "metadata": {
          "name": "Upload File",
          "path": "./src/nodes/upload/Base.tsx",
          "applet": {
            "path": "./src/nodes/upload/Applet.tsx"
          },
          "num_inputs": 0,
          "num_outputs": 1
        },
        "extension": {
          "name": "basics",
          "prod": true,
          "type": "React",
          "tag": "latest",
          "id": 20,
          "store": false
        },
        "remote": {
          "module": "Upload File",
          "scope": "testing",
          "url": "https://static.236409319020.oloren.aws.olorencore.com/web/react/basicsaf4f82cb-18a0-4995-b375-76955354c302/remoteEntry.js"
        },
        "retries": 2,
        "lastNodeId": 76325,
        "log_example": {
          "0": "pdf"
        }
      },
      "position": {
        "x": 200,
        "y": 200
      },
      "type": "RemoteNode",
      "width": 254,
      "height": 96,
      "selected": false,
      "dragging": false
    },
    {
      "id": "node-1",
      "data": {
        "data": [
          {
            "mode": "input",
            "value": "SPECIALNULLVALUEDONOTSETEVER"
          }
        ],
        "num_inputs": 1,
        "num_outputs": 1,
        "operator": "https://qlroxye6wq3y47l5bmsl4akfba0jzemj.lambda-url.us-east-1.on.aws//operator/pdf2txt",
        "hierarchy": [],
        "logs": "",
        "status": "finished",
        "outputs": [
          "EXTENDING CONTEXT WINDOW OF LARGE LAN-\nGUAGE MODELS VIA POSITION INTERPOLATION\nShouyuan Chen Sherman Wong Liangjian Chen Yuandong Tian\nMeta Platforms Inc.\n{chenshouyuan,shermanwong,clj,yuandong }@meta.com\nABSTRACT\nWe present Position Interpolation (PI) that extends the context window sizes of\nRoPE-based (Su et al., 2021) pretrained LLMs such as LLaMA (Touvron et al.,\n2023) models to up to 32768 with minimal fine-tuning (within 1000 steps), while\ndemonstrating strong empirical results on various tasks that require long context,\nincluding passkey retrieval, language modeling, and long document summariza-\ntion from LLaMA 7B to 65B. Meanwhile, the extended model by Position Inter-\npolation preserve quality relatively well on tasks within its original context win-\ndow. To achieve this goal, Position Interpolation linearly down-scales the input\nposition indices to match the original context window size, rather than extrapo-\nlating beyond the trained context length which may lead to catastrophically high\nattention scores that completely ruin the self-attention mechanism. Our theoretical\nstudy shows that the upper bound of interpolation is at least ∼600×smaller than\nthat of extrapolation, further demonstrating its stability. Models extended via Po-\nsition Interpolation retain its original architecture and can reuse most pre-existing\noptimization and infrastructure.\n1 I NTRODUCTION\nLarge language models (LLMs) typically come with a pre-defined context window size. For exam-\nple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\ncontext window limit is frequently exceeded in applications such as conducting long conversations,\nsummarizing long documents, or executing long-term planning. For these applications, LLMs with\nlonger context windows are preferred. However, training an LLM from scratch with long context\nwindows requires significant investments. This naturally leads to a question: Can we extend the\ncontext window of an existing pre-trained LLM?\nOne straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\ntext window. However, empirically, we found that models trained this way adapt to long context\nwindows very slowly. After training for more than 10000 batches, the effective context window\nsaw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\ninefficient for extending to substantially longer context windows.\nWhile certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\nextrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\nmany existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\nthat have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\nof these techniques for extending the context window sizes of such LLMs remains limited.\nIn this work, we introduce Position Interpolation to enable context window extensions for certain\nexisting pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\ndown-scale the position indices so that the maximum position index matches the previous context\nwindow limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\nmodate more input tokens, we interpolate the position encodings at neighboring integer positions,\nutilizing the fact that position encodings can be applied on non-integer positions, as opposed to\nextrapolating outside the trained positions, which may lead to catastrophic values. We verify our\napproach theoretically, by showing that the interpolated attention score has a much smaller upper\n1arXiv:2306.15595v2  [cs.CL]  28 Jun 2023\nNormalPre-trained range\nPre-trained rangeUnseen Range\nPosition Interpolationf’(x, m) = f(x, m/2)ExtrapolationFigure 1: An illustration of our Position Interpolation method. Consider a Llama model pre-trained with a\n2048 context window length. Upper left illustrates the normal usage of an LLM model: input position indices\n(blue dots) are within the pre-trained range. Upper right illustrates length extrapolation where models are\nrequired to operate unseen positions (red dots) up to 4096. Lower left illustrates Position Interpolation where\nwe downscale the position indices (blue and green dots) themselves from [0, 4096] to [0, 2048] to force them\nto reside in the pretrained range.\nbound ( ∼600×smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\nstable. Therefore, interpolated position encodings are easier for the model to adapt.\nEmpirically, we found that Position Interpolation is highly effective and efficient, requiring only a\nvery short period of fine-tuning for the model to fully adapt to greatly extended context windows.\nWe present experimental results for extending the context window to up to 32768 from the initial\n2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\nonly fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\nThe cost of fine-tuning is negligible compared to the pre-training costs. This confirms\nour hypothesis that it is relatively easy for the models to adapt to interpolated position\nencodings.\n2. Position Interpolation generates strong models that can effectively make use of much ex-\ntended context window. We show that models extended by Position Interpolation enjoy\nsignificant perplexity gains from greatly extended context windows for text modeling, and\nwe show that the perplexity reduces graceful with the enlargement of context windows.\nWe also applied Position Interpolation in a long text summarization task, and demonstrate\ncompetitive performances.\n3. Position Interpolation preserves model quality relatively well for tasks within its original\ncontext window sizes. We present a variety of evaluation results for the extended LLaMA\nmodels on the original LLaMA benchmark. Compared with original LLaMA models, the\nextended LLaMA models saw a minor degradation on several standard benchmarks within\na 2048 token limit.\nOur results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\nlonger than the ones encountered during training” as hypothesized in the seminal work of Vaswani\net al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\ntrapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n2\nextrapolation of positional encodings and it can be largely mitigated by interpolating position en-\ncodings instead.\nConcurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\nHOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\nwindow from 2K to 8K. Recently, open source community picks it up in Reddit post1and Github\nIssues2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\npaper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\nalso give theoretical explanations why interpolation achieves much more stable results than extrap-\nolation, by showing that the upper bound of interplated attention score is much lower than that of\nextrapolated ones.\n2 M ETHOD\n2.1 B ACKGROUND : ROTARY POSITION EMBEDDING (ROPE)\nTransformer models require explicit positional information to be injected, typically in the form of\npositional encodings, to represent the order of inputs. We consider Rotary Position Embedding\n(RoPE) (Su et al., 2021), which is the position encoding used in the LLaMA model (Touvron et al.,\n2023). Given a position index m∈[0, c)and an embedding vector x:= [x0, x1, . . . , x d−1]⊤, where\ndis the dimension of the attention head, RoPE defines a vector-valued complex function f(x, m)as\nfollows\nf(x, m) = [( x0+ ix1)eimθ0,(x2+ ix3)eimθ1, . . . , (xd−2+ ixd−1)eimθd/2−1]⊤(1)\nwhere i :=√−1is the imaginary unit and θj= 10000−2j/d. Using RoPE, the self-attention score\na(m, n) = Re ⟨f(q, m),f(k, n)⟩\n= Re\nd/2−1X\nj=0(q2j+ iq2j+1)(k2j−ik2j+1)ei(m−n)θj\n\n=d/2−1X\nj=0(q2jk2j+q2j+1k2j+1) cos(( m−n)θj) + (q2jk2j+1−q2j+1k2j) sin(( m−n)θj)\n=:a(m−n) (2)\nis only dependent on relative position m−nthrough trigonometric functions. Here qandkare the\nquery and key vector for a specific attention head. At each layer, RoPE is applied on both query and\nkey embeddings for computing attention scores.\n2.2 D IRECT EXTRAPOLATION\nWhile the attention score in RoPE only depends on the relative positions, which is what we want,\nits extrapolation performance is not great . In particular, when directly extending to larger context\nwindows unseen in the training, the perplexity may shoot up to very high numbers (i.e., >103),\ncomparable to untrained models.\nIdeally, we want to see the model trained on a context window of size L= 2048 to still work\nreasonably well on longer context window, but may not have the capability to leverage information\nthat appears beyond L. For example, to answer a question located at 3000, the model trained on\nmaximal window size of L= 2048 cannot leverage evidences provided at location 0, but still\ncan leverage the evidences provided at location 2900. In contrast, in reality we see catastrophic\nbehaviors, i.e., question at location 3000 cannot be answered correctly, even if the evidences are\nlocated at location 2900.\nWhat is the reason behind? How could this happen if the attention score am−ndecays as the relative\ndistance |m−n|increases, according to Section 3.4.3 of (Su et al., 2021), and content from very\n1https://www.reddit.com/r/LocalLLaMA/comments/14fgjqj/a_simple_way_to_\nextending_context_to_8k/\n2https://github.com/ggerganov/llama.cpp/discussions/1965\n3\n0 500 1000 1500 2000\nPositional difference s3\n2\n1\n0123attention score a(s)\n0 1000 2000 3000 4000\nPositional difference s02000400060008000Effect of Extrapolation\n30 40 50 60 70\nPositional difference s0.2\n0.1\n0.00.10.2Effect of InterpolationFigure 2: Extrapolation versus interpolation. Left: a fitted attention score function (in red) in the form of\nEqn. 3 with d=dmodel/nhead= 4096 /32 = 128 (setting of LLaMA 7B). Dots are random input points to be\nfitted and red curve is the fitted score function via least square, which is approximately within [−1,1].Middle:\nWhile the fitted function seems to be well bounded in [0, L], where L= 2048 , out of this region it may goes\nbeyond 8000 , causing catastrophic issues in attention computation. Note that here we do not cherry pick at all:\nalmost every learned curve from a set of randomly generated input points within [0, L]has the extrapolation\nissue. Right: On the other hand, interpolation is much more stable. Curves in between vertical dotted lines\n(i.e., integer positional difference) are smooth and well-behaved. Please check Appendix C.1 for the source\ncode used to generate the figure.\nfar distances should not matter that much? It turns out that the upper bound derived in Section 3.4.3\nof (Su et al., 2021) may be too loose: while it indeed decays with respect to |m−n|, the bound\ncan still be quite large (i.e., the bound can be critically depends on the magnitude of vj) and thus\nvacuous. In fact, if we treat all trigonometric functions as basis functions (i.e, ϕj(s) :=eisθj), and\nthink about Eqn. 2 as basis expansion as the following:\na(s) = Re\nd/2−1X\nj=0hjeisθj\n (3)\nwhere sis the positional span between a query and a key and hj:= (q2j+ iq2j+1)(k2j−ik2j+1)\nare complex coefficients depending on qandk(here the definition of hjis exactly the same as the\ndefinition of hjin Sec 3.4.3 in RoPE (Su et al., 2021)). Now the the issue becomes clear: as shown\nin Fig. 2, ascan be small in magnitude in the range of [0,2048] , but gives huge values out of the\nregion. The underlying reason is that the trigonometric family {ϕj}(with sufficiently large d) is\na universal approximator and can fit any arbitrary functions. Therefore, for as, there always exist\ncoefficients {hj}(i.e. key and query) that corresponds to small function values in [0, 2048] but\nmuch larger in regions beyond.\n2.3 P ROPOSED APPROACH : POSITION INTERPOLATION (PI)\nIn Fig. 2, thanks to the smoothness of bases functions ϕjinterpolation is much more stable and will\nnot lead to wild values. Therefore, instead of extrapolate the attention score in Eqn. 3 to s > L ,\nhow about we define an attention score ˜a(s) =a(Ls/L′)where L′is the longer context window?\nFormally, we replace RoPE fbyf′defined as follows\nf′(x, m) =f\u0012\nx,mL\nL′\u0013\n. (4)\nWe call this transformation on the position encoding Position Interpolation . In this step, we reduce\nposition indices from [0, L′)to[0, L)to match the original range of indices before computing RoPE.\nConsequently, as inputs to RoPE, the maximum relative distance between any two tokens has been\nreduced from L′toL. Since we align the ranges of position indices and relative distances before\nand after extension, we mitigate the effect on attention score computation due to context window\nextensions, which can allow the model easier to adapt. To further demonstrate this is the case, in the\nfollowing theorem, we show that the interpolated attention score is well-behaved:\n4\nTheorem 2.1 (Interpolation bound) .For attention score a(s) = RehPd/2−1\nj=0hjeisθji\n, where θj=\nc−2j/d, its interpolation value a(s)fors∈[s1, s2]is bounded as follows:\n|a(s)−alinear(s)| ≤d\u0012\nmax\nj|hj|\u0013(s−s1)(s2−s)\n8 lnc(5)\nwhere alinear(s)is the linear interpolation of two grid point a(s1)anda(s2)that are known to\nbehave well, enforced by LLM pre-training:\nalinear(s) := (1 −λ(s))a(s1) +λ(s)a(s2), λ (s) :=s−s1\ns2−s1(6)\nPlease check Appendix A for the proof. Intuitively, in LLM pre-training, we know that the attention\nscore a(s)behaves well on integer grid s1ands2. Therefore, for any interpolation s∈[s1, s2], we\nhave (s−s1)(s2−s)≤1/4. Note that c= 10000 , the bound becomes:\n|a(s)−alinear(s)| ≤d\n32 lncmax\nj|hj| ≈dmax j|hj|\n294.73(7)\nIn comparison, Sec. 3.4.3 in RoPE (Su et al., 2021) yields an extrapolation bound (i.e., it works for\nall positional distance s):\n|a(s)| ≤\u0012\nmax\nj|hj−hj+1|\u0013d/2−1X\nk=0|Ak+1(s)| ≤2\u0012\nmax\nj|hj|\u0013d/2−1X\nk=0|Ak+1(s)|, (8)\nwhere Ak(s) :=Pk−1\nj=0eisθj. While there is no close form for B(s) :=Pd/2−1\nk=0|Ak+1(s)|, numer-\nically it is at least larger than d, and for many positional difference s,B(s)is much larger than d\n(check Appendix B for the plot). Therefore, the interpolation bound is at least 2·294.73∼600×\nsmaller than the extrapolation bound, and thus the interpolated attention score is much more stable\nthan extrapolated one.\nNotably, our method of rescaling of position indices does not introduce extra weight, or modify\nthe model architecture in any way. This makes it attractive in practical applications, since most\ninfrastructure and optimization for the original model can be reused after the extension.\nFine-tuning. We can further fine-tune the interpolated model using the next token prediction task\nwith interpolated position encodings on the extended context window size using a pre-training cor-\npus such as the Pile (Gao et al., 2020). In the next section, we show that our fine-tuning process\nonly needs tens to hundreds thousands of examples. We also find that the result of the fine-tuning\nis not sensitive to the choice of examples. The reason may be that the model is only adapting to the\nnew context window during the fine-tuning phase, starting from a good initialization, as opposed to\nacquiring new knowledge.\nOther ways to reduce interpolation/extrapolation bound. From the expression of the interpola-\ntion (Eqn. 5) and extrapolation bound (Eqn. 8), a common term is max j|hj|, which is the maximal\nmagnitude of query/key products. If we enforce a regularization on |hj|during LLM training, it is\npossible that the catastrophic extrapolation error can be mitigated or even resolved. In fact, if we\napply ridge regression with proper regularization to fit a curve in Fig. 2, the magnitude of extrapo-\nlateda(s)when s > L can be comparable to that within [0, L]. To our knowledge, we are not aware\nof existing LLM pre-training techniques that leverage this regularization and will leave it for future\nwork.\n3 E XPERIMENTS\nWe show Position Interpolation can effectively extend context window up to 32 times of the original\nsize, and such extension can be done with only several hundreds of training steps. We show the\nresulting models are strong LLMs with fully effective long context windows. We demonstrate its\nperformance in a number of tasks including language modeling, passkey retrieval, and long doc-\nument summarization. We also present benchmark results of the extended models on the original\nLLaMA evaluation benchmarks.\n5\n3.1 S ETUP\nModel Variants . We extended the pre-trained 7B, 13B, 33B and 65B LLaMA models (Touvron\net al., 2023) to various context window of sizes up to 32768, using either direct fine-tuning or\nPosition Interpoloation method. Except for rescaling the position indices for models extended with\nPosition Interpolation, we did not modify LLaMA model architectures (Touvron et al., 2023) in any\nways.\nTraining Procedure . We fine-tune all model variants using the next token prediction objective. We\nuse AdamW (Loshchilov & Hutter, 2019) with β1= 0.9andβ2= 0.95. We use a linear learning\nrate warmup of 20 steps starting from 10% of the maximum learning rate. For 7B and 13B models,\nwe set the learning rate to 2×10−5and for 33B and 65B models we set the learning rate to 10−5. We\nset the weight decay to zero. For extending 7B, 13B and 33B models to the 8192 context window\nsize, we use 32 A100 GPUs and 64 global batch size. For all other cases we use 128 A100 GPUs and\n128 global batch size. We note that the main need of using more GPUs is memory limitation during\nfine-tuning, and it is possible to use fewer GPUs in certain cases. We train all models using PyTorch\n(Paszke et al., 2019) with Fully Sharded Data Parallel (Zhao et al., 2023) and Flash Attention (Dao\net al., 2022).\nIf not specified otherwise, for the Position Interpolation method, we fine-tune the models for 1000\nsteps. For the direct fine-tuning method, we use 10000 steps. We primarily fine-tune using the Pile\ntraining dataset (Gao et al., 2020). In Section 3.4 we also compared fine-tuning performance on the\nRedPajama dataset (Computer, 2023).\n3.2 L ONG SEQUENCE LANGUAGE MODELING\nWe evaluate the long sequence language modeling performance of our extended models and base-\nlines on two datasets: book corpus (PG-19) (Rae et al., 2020) and cleaned Arxiv Math proof-pile\ndataset (Azerbayev et al., 2022).\nWe use the test splits of PG19 (Rae et al., 2020) and proof-pile (Azerbayev et al., 2022). For PG19,\nwe use the whole test split consisting of 100 documents. For the proof-pile dataset, we use a random\nsubsample of 128 documents with at least 32768 SentencePiece (Kudo & Richardson, 2018) tokens\nand truncate to the first 32768 tokens for each test document. We evaluate perplexity at various\ncontext window size by using a sliding window approach following Press et al. (2022) with stride\nS= 256 .\nIn Table 1 and Table 2, we report the perplexity results for our models and baselines on the datasets.\nFrom the results, we found that models extended with our method enjoy a significantly improved\nperplexity from longer context window sizes. By increasing the context window size from 2048 to\n16384, we observed -0.28 and -0.5 reductions of perplexity for extending LLaMA 7B models on\nboth datasets, -0.27 and -0.48 reductions for extending LLaMA 13B models, and -0.14 and -0.42\nreductions for extending LLaMA 33B models. For LLaMA 65B models, we observed -0.12 and\n-0.3 reductions of perplexity by extending to the 8192 context window size.\nIn general, we observed a consistent trend of our models achieving better perplexity with longer\ncontext windows. This indicates our models can effectively make use of the longer context windows\nto better predict next tokens in language modeling tasks. Moreover, we found this trend extends to\n32768 window size without diminishing on the PG19 dataset for LLaMA 7B and 13B models. This\nindicates that our method may enable extension to even longer context windows.\nIn contrast, we observed that models extended via the direct fine-tuning method has shown regres-\nsion (up to +0.48) or minor improvement (up to -0.12) on the perplexity at longer context windows.\nThis indicates that models extended this way have limited capability of making use of context win-\ndows longer than their pre-trained settings.\nWe saw a minor degradation of the perplexity on the original context window of 2048 for our ex-\ntended models in some cases. For example, on the Proof-pile dataset, we saw a degradation ranging\nfrom 0.01 to 0.05 across all models with extended with Position Interpolation. A small degradation\nof performance within original evaluation context window is expected since Position Interpolation\nforces position encodings in original context window to reside in a much narrower region, which\n6\nmay negatively affect the language model’s performance. We present more benchmark results on\nthe original context window size in Section 3.4.\nIn Table 3 we report the relationship between perplexity and the number of fine-tuning steps for\nLLaMA 7B model extending to 8192 and 16384 context window sizes using Position Interpolation\nevaluated on the PG19 dataset. We can see without fine-tuning (at step 0) the model can exhibit\ncertain language modeling capability, as indicated by <20perplexity for extending to 8192 context\nwindow (in contrast, the direct extrapolation method leads to >103perplexity). With fine-tuning,\nwe observed that the perplexity improves quickly. At 200 steps the models surpassed the original\nmodel’s perplexity on 2048 context window size, indicating the models gaining ability of effectively\nusing sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\nsee the models have improved steadily and achieve a significantly better perplexity.\nModel Evaluation Context Window Size\nSize Context Window Method 2048 4096 8192 16384 32768\n7B 2048 None 7.20 >103>103>103>103\n7B 8192 FT 7.21 7.34 7.69 - -\n7B 8192 PI 7.13 6.96 6.95 - -\n7B 16384 PI 7.11 6.93 6.82 6.83 -\n7B 32768 PI 7.23 7.04 6.91 6.80 6.77\n13B 2048 None 6.59 - - - -\n13B 8192 FT 6.56 6.57 6.69 - -\n13B 8192 PI 6.55 6.42 6.42 - -\n13B 16384 PI 6.56 6.42 6.31 6.32 -\n13B 32768 PI 6.54 6.40 6.28 6.18 6.09\n33B 2048 None 5.82 - - - -\n33B 8192 FT 5.88 5.99 6.21 - -\n33B 8192 PI 5.82 5.69 5.71 - -\n33B 16384 PI 5.87 5.74 5.67 5.68 -\n65B 2048 None 5.49 - - - -\n65B 8192 PI 5.42 5.32 5.37 - -\nTable 1: Evaluation perplexity on PG19 dataset (Rae et al., 2020). FT: Direct Fine-tuning. PI: Position\nInterpolation. Model fine-tuned with PI shows progressively lower perplexity with longer context window,\nshowing that PI can leverage long context well, while the perplexity of FT increases over longer window. Note\nthat overall the perplexity is higher compared to Table 2 since PG19 has very different writing styles.\n3.3 M EASURING EFFECTIVE CONTEXT WINDOW SIZE THROUGH PASSKEY RETRIEVAL\nWe study the effective context window size, i.e. the maximum distance of a token can effectively\nattend to during inference, of our models after extension. To measure this, we follow a synthetic\nevaluation task of passkey retrieval proposed by Mohtashami & Jaggi (2023). In this task, the models\nare asked to recover a random passkey hidden in a long document. See Figure 3 for the format of\nthe document.\nGiven a language model, we estimate the upper and lower bounds of effective context windows as\nfollows. Suppose the random passkey is ktokens away from the end of the input. When a model\npersistently fails to retrieve the correct passkey value across several independent attempts, it suggests\nthat the effective context window size of the model is less than k. Conversely, if a model consistently\nsucceeds in retrieving the correct passkey value, we deduce that the effective context window size\nof the model is at least k.\nWe evaluate the 7B and 33B LLaMA model variants that are extended via Position Interpolation or\ndirect fine-tuning. For each model, we use 32 different kuniformly spaced in the targeted context\nwindow L′and run the above tests for 10 times for each k, where each time a random passkey of 5\nrandom digits is used. In Table 4, we report kmaxas a function of the number of fine-tuning steps,\n7\nModel Evaluation Context Window Size\nSize Context Window Method 2048 4096 8192 16384 32768\n7B 2048 None 2.77 - - - -\n7B 8192 FT 2.85 2.74 2.73 - -\n7B 8192 PI 2.79 2.57 2.39 - -\n7B 16384 PI 2.79 2.57 2.37 2.25 -\n7B 32768 PI 2.82 2.59 2.39 2.24 2.48\n13B 2048 None 2.66 - - - -\n13B 8192 FT 2.71 2.56 2.50 - -\n13B 8192 PI 2.67 2.47 2.30 - -\n13B 16384 PI 2.68 2.47 2.29 2.18 -\n13B 32768 PI 2.68 2.46 2.28 2.15 2.35\n33B 2048 None 2.49 - - - -\n33B 8192 FT 2.56 2.48 2.47 - -\n33B 8192 PI 2.50 2.32 2.18 - -\n33B 16384 PI 2.53 2.34 2.18 2.07 -\n65B 2048 None 2.42 - - - -\n65B 8192 PI 2.43 2.26 2.12 - -\nTable 2: Evaluation perplexity on Arxiv Math Proof-pile dataset (Azerbayev et al., 2022). FT: Direct Fine-\ntuning. PI: Position Interpolation.\nModel Number of fine-tuning steps\nSize Context Window 0 200 400 600 800 1000\n7B 8192 16.10 7.12 7.10 7.02 6.99 6.95\n7B 16384 112.13 7.05 6.93 6.88 6.84 6.83\nTable 3: Evaluation perplexity on PG19 dataset (Rae et al., 2020) with respect to the number of fine-tuning\nsteps using Position Interpolation.\n8\nwhere kmaxis defined as the maximum ksuch that, for all k′≤k, the model has a success rate of at\nleast 20% on k′.\nWe can see that models extended via Position Interpolation all successfully attain their desired ex-\ntension objectives in terms of effective context window sizes, indicating by the effective context\nwindow size reaching maximum kmax=L′, after merely fine-tuning for 200 steps, consistently\nacross both 7B and 33B model sizes and up to 32768 context windows. In contrast, LLaMA models\nthat are extended via direct fine-tuning only saw a minimal increase of the effective context win-\ndow size kmaxfrom 2048 to 2560, even after fine-tuning for more than 10000 steps, with no clear\nindication of an acceleration in the increase of window size.\nModel Fine-tuning steps\nSize Context Window Method 200 400 600 800 1000 10000\n7B 8192 FT 1792 2048 2048 2048 2304 2560\n33B 8192 FT 1792 2048 1792 2048 2304 -\n7B 8192 PI 8192 8192 8192 8192 8192 -\n7B 16384 PI 16384 16384 16384 16384 16384 -\n7B 32768 PI 32768 32768 18432 32768 32768 -\n33B 8192 PI 8192 8192 8192 8192 8192 -\n33B 16384 PI 16384 16384 16384 16384 16384 -\nTable 4: Effective context window sizes after fine-tuning. FT: Direct fine-tuning. PI: Position Interpolation.\nThere is an important info hidden inside a lot of irrelevant text. Find\nit and memorize them. I will quiz you about the important information\nthere.\nThe grass is green. The sky is blue. The sun is yellow. Here we go.\nThere and back again. (repeat X times)\nThe pass key is 12345. Remember it. 12345 is the pass key.\nThe grass is green. The sky is blue. The sun is yellow. Here we go.\nThere and back again. (repeat Y times)\nWhat is the pass key? The pass key is\nFigure 3: Prompt format for passkey retrieval. We use the exact same prompt as proposed by Mohtashami &\nJaggi (2023). Here the passkey 12345 is replaced with a random 5-digit numbers during test.\n3.4 B ENCHMARKS ON ORIGINAL CONTEXT WINDOW SIZE\nWe evaluate the models extended by Position Interpolation on several standard benchmark tasks\nwithin the original context window size of 2048. The evaluation results are listed in Table 5. From\nthe results, we saw that models extended to 8192 produce comparable results on the original bench-\nmark which is designed for a much smaller context window, with a degradation of up to 2% on\nthe benchmark tasks, for both 7B and 33B model sizes. Models extended to longer context win-\ndows regressed more on the benchmarks, but still in reasonable ranges for most tasks. We also note\nthat the choice of fine-tuning datasets does not seem to lead significant difference in the benchmark\nperformances, which may be due to the limited number of fine-tuning steps used in our method.\nThe regression on benchmark tasks is consistent with our observation on perplexity regression in\nSection 3.2.\n3.5 L ONG DOCUMENT SUMMARIZATION\nIn this task, we evaluate our models’ performance on the long document summarization task. In\nparticular, we consider the GovReport (Huang et al., 2021) dataset, which contains 17457 documents\nfor training and 972 documents for evaluation. Each document comes with a human generated\nsummary. We truncate all input documents to their first 15000 tokens.\nWe fine-tune the LLaMA models extended with Position Interpolation with a context window of\n16384. Note the rescaling of position indices are still required during this fine-tuning step. We first\n9\nModel Size Context Window Fine-tune on BoolQ PIQA Race-M Race-H WinoGrande\n7B 2048 None 76.1 78.9 55.7 42.2 69.6\n7B 8192 Pile 73.2 78.2 53.8 41.7 69.0\n7B 16384 Pile 69.8 77.6 53.3 40.9 67.8\n7B 32768 Pile 64.7 77.2 50.1 39.6 66.9\n7B 8192 RedPajama 75.5 77.4 54.5 41.5 68.1\n33B 2048 None 81.6 80.2 61.1 45.9 76.2\n33B 8192 Pile 80.2 80.7 60.2 45.7 75.9\nTable 5: Zero-shot performance on a subset of LLaMA Benchmarks. Models extended by Position Interpola-\ntion comparable performance as the original models, except for BoolQ dataset that may require models to pay\nclose attention to word ordering in a short reference paragraph.\nModel Evaluation Score\nModel Context Window ROUGE-1 ROUGE-2 ROUGE-L\nCoLT5 Base (Ainslie et al., 2023) 16K 58.7 29.6 31.4\nCoLT5 XL (Ainslie et al., 2023) 16K 61.3 32.2 33.8\nLLaMA-7B Extended 16K 60.0 28.0 29.5\nTable 6: ROUGE Score on GovReport Dataset.\nformat the raw document using the prompt template in Figure 4, and then concatenate the prompt\nwith the ground-truth summary (truncate to 1000 tokens) associated with each document. We fine-\ntune the model using the next token prediction task with the above setup for 10 epochs. The losses\nfrom the input prompt proportion of training examples are excluded during our fine-tuning.\nWe use a generation temperature of 0.5 and topp= 0.95as our inference parameter to generate a\nsummarization of each document in the test set. The final output is truncated at 1000 tokens. We\nused the ROUGE-1/ROUGE-2/ROUGE-L scores (Lin, 2004) as the evaluation metrics to evaluate\nthe models’ outputs vs the ground-truth summaries.\nIn Table 6 we report our evaluation results. We have also included results from two baselines in\nexisting SCROLLS Leaderboard (Shaham et al., 2022; Ainslie et al., 2023). In general, we have\nobtained competitive R1 score among other models with minimal tuning of hyper-parameters. This\nresult suggests our models with 16384 context window can effectively handle the long document\nsummarization task.\nRead the following article and then summarize it.\n# .... Document goes here\nNow summarize the above article.\nSummary:\nFigure 4: Input format for long doc summarization.\n4 R ELATED WORK\nRetrieval-augmented LLM. One line of work extends LLMs by augmenting it with retrieval mod-\nules which fetch related documents and include the retrieval results into the input context of an LLM\n(Karpukhin et al., 2020; Guu et al., 2020; Izacard et al., 2022; Jiang et al., 2022; Khattab et al., 2021;\nSanthanam et al., 2022). Our work is complementary to these works as our extended context win-\ndow allows more documents being included in the input. In addition, with an unmodified attention\nmechanism and model architecture, our method may be more versatile as it can natively handle tasks\nbeyond retrieval oriented ones, such as long document summarization, few-shots learning, etc.\n10\nRecurrent Transformers and Memory Transformers. Several works add memory capabilities\nto Transformers through recurrence, which increase the models’ capability of handling very long\nsequences (Bulatov et al., 2022; Wu et al., 2020; Dai et al., 2019; Wu et al., 2022; Martins et al.,\n2021; Mu et al., 2023). One limitation of these works is that they only allow attending to a lossy\ncompressed version of past inputs. Mu et al. (2023) suggested that this may prevent models from\nremembering specific details in the past inputs. In contrast, our work allows attending to all previous\ntokens, preserving all details without compression, albeit with higher inference costs. Mohtashami\n& Jaggi (2023) proposed landmark attention which allows full random access to any chunk of the\ninput through introducing landmark tokens. Our work allows full access of the entire input through\nunmodified attention, which may be useful for tasks such as summarization.\nApproximated Multi-head Attention. There is a large body of research that focuses on decreasing\nthe memory and computational complexity of the multi-head attention (MHA) mechanism through\napproximation or sparsification (Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020; Wang\net al., 2020; Choromanski et al., 2021; Kitaev et al., 2020; Ren et al., 2021). Although not the focus\nof this work, as these methods are not used in LLaMA (Touvron et al., 2023), we note that our\nmethod is compatible with most of them since our changes are restricted to position encodings, and\nnot attention mechanisms.\nLength Extrapolation. A recent line of research aims to train Transformers models on short se-\nquences and inference on longer (Press et al., 2022; Sun et al., 2022; Haviv et al., 2022). However,\nthese methods have not been applied in some of the largest language models such as LLaMA (Tou-\nvron et al., 2023), or OPT (Zhang et al., 2022). This has prevented them from enabling length\nextrapolation of many pre-existing pre-trained language models. Our work focuses on extending\nexisting LLMs, which can save substantial pre-training costs. In addition, our method preserves the\nquality of the original models, even for small context window tasks, since it does not deviate far\nfrom existing definitions of position encoding or attention mechanisms.\nInterpolation. The most related technique to ours is proposed by Dosovitskiy et al. (2021) in their\nwork on Vision Transformers, where the authors proposed to linearly interpolate learnt position em-\nbeddings to support higher resolution, which translates to an increased number of input embeddings,\nin the fine-tuning stage. The interpolated position embedding weights are used as initialization in the\nfine-tuning process for the newly added positions. Our work differs from their work in several ways\n(1) Instead of interpolating position embeddings, our method interpolates position indices, which\nis more suitable for RoPE like position encodings and may require less training since no trainable\nparameters are added. (2) We report successful results of extending the context window to 32 times\nwhile Dosovitskiy et al. (2021) explored up to 4 times. Our results extend theirs in exploring the\nupper limit of context window extension via interpolation. (3) We evaluated and confirmed the\neffectiveness of Position Interpolation for extending context windows for language models.\nWe believe our results, in conjunction with (Dosovitskiy et al., 2021), provide empirical evidence\non Transformer’s remarkable ability of handling significantly longer sequences beyond training.\nFurther, we conjecture that a method similar to theirs is directly applicable in LLMs with learnable\nposition embeddings such as OPT (Zhang et al., 2022) and we plan to investigate this in the future.\n5 C ONCLUSIONS\nPosition Interpolation can effectively extend LLaMA models’ context window to be significantly\nlarger, using minimal fine-tuning. The extended models are fully capable to perform a variety of\ntasks on the extended context windows, and preserve its original ability relatively well for tasks\nwithin the original extended models, making them good choices of generic language models for\nboth long and short input prompts. Further, models extended by Position Interpolation can reuse\nmost pre-existing infrastructure and optimization, making this method attractive in many practical\napplications. We believe that Position Interpolation is a general method that could be apply to other\ntypes of position encodings, which can allow extension for more types of LLMs, and we plan to\ninvestigate in such directions in the near future.\n11\nACKNOWLEDGEMENTS\nWe thank Mike Lewis for his input on evaluation.\nREFERENCES\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta ˜n´on, Siddhartha Brahma, Yury Zemlyanskiy,\nDavid Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-Hsuan Sung, and Sumit Sanghai. Colt5:\nFaster long-range transformers with conditional computation, 2023.\nZhangir Azerbayev, Edward Ayers, and Bartosz Piotrowski. Proof-pile, 2022. URL https://\ngithub.com/zhangir-azerbayev/proof-pile .\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\n2020.\nAydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer. 2022.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. 2019.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea\nGane, Tam ´as Sarl ´os, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser,\nDavid Benjamin Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with per-\nformers. In 9th International Conference on Learning Representations, ICLR 2021 . OpenRe-\nview.net, May 2021.\nTogether Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023.\nURLhttps://github.com/togethercomputer/RedPajama-Data .\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformerxl: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics , pp. 2978–2988, Florence,\nItaly, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285.\nTri Dao, Daniel Y . Fu, Stefano Ermon, Atri Rudra, and Christopher R ´e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-\ning Systems , 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations , 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy .\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 , 2020.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training. 2020.\nAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer Levy. Transformer language models without\npositional encodings still learn positional information. 2022.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 , 2021.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for\nlong document summarization. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies , pp.\n1419–1436, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.naacl-main.112. URL https://aclanthology.org/2021.naacl-main.112 .\n12\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning\nwith retrieval augmented language models. 2022.\nZhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neu-\nbig. Retrieval as attention: End-to-end learning of retrieval and reading within a single trans-\nformer. 2022.\nkaiokendev. Things i ´m learning while training superhot. https://kaiokendev.github.\nio/til#extending-context-to-8k , 2023.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP) , pp. 6769–6781. Association for Computational Linguistics, 2020. doi: 10.18653/\nv1/2020.emnlp-main.550.\nOmar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with\ncolbert. Transactions of the Association for Computational Linguistics , 9:929–944, 2021. doi:\n10.1162/tacl a00405.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th\nInternational Conference on Learning Representations, ICLR 2020 . OpenReview.net, April 2020.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Processing: System Demonstrations , pp.\n66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi:\n10.18653/v1/D18-2012. URL https://aclanthology.org/D18-2012 .\nChin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out , pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguis-\ntics. URL https://aclanthology.org/W04-1013 .\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations , 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7 .\nPedro Henrique Martins, Zita Marinho, and Andr ´e F. T. Martins. ∞-former: Infinite memory trans-\nformer. 2021.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. arXiv preprint arXiv:2305.16300 , 2023.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens.\n2023.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K ¨opf, Ed-\nward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance\nDeep Learning Library . Curran Associates Inc., Red Hook, NY , USA, 2019.\nOfir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables\ninput length extrapolation. In International Conference on Learning Representations , 2022. URL\nhttps://openreview.net/forum?id=R8sQPpGCv0 .\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lilli-\ncrap. Compressive transformers for long-range sequence modelling. In International Confer-\nence on Learning Representations , 2020. URL https://openreview.net/forum?id=\nSylKikSYDH .\n13\nHongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and\nBo Dai. Combiner: Full attention transformer with sparse computation cost. In Marc’Aurelio\nRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan\n(eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021 , pp. 22470–22482. Curran Associates, Inc.,\n2021.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. Col-\nbertv2: Effective and efficient retrieval via lightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies , pp. 3715–3734, Seattle, United States, 2022. Association\nfor Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.272.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long\nlanguage sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing , pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2022.\nemnlp-main.823 .\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding, 2021.\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaud-\nhary, Xia Song, and Furu Wei. A length-extrapolatable transformer, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V on\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-\nvances in Neural Information Processing Systems , volume 30. Curran Associates, Inc.,\n2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/\nfile/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. 2020.\nQingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu. Memformer:\nA memory-augmented transformer for sequence modeling. 2020.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing trans-\nformers. In The Tenth International Conference on Learning Representations, ICLR 2022 . Open-\nReview.net, April 2022.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-\nago Onta ˜n´on, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird:\nTransformers for longer sequences. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,\nMaria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020 .\nCurran Associates, Inc., 2020.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022.\nYanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright,\nHamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard Nguyen,\nGeeta Chauhan, Yuchen Hao, and Shen Li. Pytorch fsdp: Experiences on scaling fully sharded\ndata parallel, 2023.\n14\nAppendix\nA P ROOF\nTheorem 2.1 (Interpolation bound) .For attention score a(s) = RehPd/2−1\nj=0hjeisθji\n, where θj=\nc−2j/d, its interpolation value a(s)fors∈[s1, s2]is bounded as follows:\n|a(s)−alinear(s)| ≤d\u0012\nmax\nj|hj|\u0013(s−s1)(s2−s)\n8 lnc(5)\nwhere alinear(s)is the linear interpolation of two grid point a(s1)anda(s2)that are known to\nbehave well, enforced by LLM pre-training:\nalinear(s) := (1 −λ(s))a(s1) +λ(s)a(s2), λ (s) :=s−s1\ns2−s1(6)\nProof. Using Taylor expansion, we have:\na(s1) = a(s) +a′(s)(s−s1) +1\n2a′′(ξ1)(s−s1)2(9)\na(s2) = a(s) +a′(s)(s−s2) +1\n2a′′(ξ2)(s−s2)2(10)\nwhere ξ1∈[s1, s]andξ2∈[s, s2]. Multiplying Eqn. 9 with s−s2and Eqn. 10 with s−s1and\nsubtract, we get:\na(s)−alinear(s) =R(s) :=−(s−s1)(s−s2)\n2(s1−s2)[a′′(ξ1)(s−s1)−a′′(ξ2)(s−s2)] (11)\nNow we bound the second order derivative a′′(s). Note that for any complex number x,|Re(x)| ≤\n|x|so we have:\n|a′′(s)| ≤d/2−1X\nj=0|hj||ϕ′′\nj(s)| ≤d/2−1X\nj=0|hj|θ2\nj (12)\n≤\u0012\nmax\nj|hj|\u0013d/2−1X\nj=0c−4j/d=\u0012\nmax\nj|hj|\u00131\n1−c−4/d(13)\nNote that when x <0andc >1,cx≤1 +xlnc, therefore c−4/d≤1−4/dlncand we have:\n1\n1−c−4/d≤1\n4/dlnc=d\n4 lnc(14)\nSo\n|a′′(s)| ≤\u0012\nmax\nj|hj|\u0013d\n4 lnc=:M (15)\nLet the above bound to be M, we have:\n|R(s)| ≤(s−s1)(s2−s)\n2(s2−s1)[M(s−s1) +M(s2−s)] =M\n2(s−s1)(s2−s) (16)\nAs a result:\n|a(s)−alinear(s)|=|R(s)| ≤d\u0012\nmax\nj|hj|\u0013(s−s1)(s2−s)\n8 lnc(17)\nB V ISUALIZATION OF QUANTITIES IN EXTRAPOLATION BOUND\nAs shown in Eqn. 8, the extrapolation bound contains the term B(s) :=Pd/2−1\nk=0|Ak+1(s)|where\nAk(s) :=Pk−1\nj=0eisθj. Here we check how large the bound is. We use θj=c−2j/dwithc= 10000\nandd= 4096 /32 = 128 (LLaMA-7B setting), and Fig. 5 shows that B(s)/dalmost always larger\nthan1and in many places it is much larger than 1.\n15\n0 1000 2000 3000 4000\nPositional difference s246810121416B(s)/dFigure 5: The bound B(s)/ddecays with s. While the bounds goes down with large positional difference\ns, numerically B(s)/d≥1and at many smuch larger than 1(the dotted horizontal line). Please check\nAppendix C.2 for the source code used to draw the figure.\n16\nC C ODE\nC.1 C ODE FOR FIG. 2\n# build basis function\nd = 4096 // 32\ntheta = 10000\n# Frequency computation,\nfreqs = 1.0 / (theta **(torch.arange(0, d, 2)[: (d // 2)].float() / d))\n# construct basis function\nL = 2048\nx = torch.zeros(L)\nx[:L] = torch.arange(0, L)\n# basis functions\nxfreq = torch.outer(x, freqs)\ny = torch.randn(x.shape[0])\n# do linear regression\nX = torch.cat([xfreq.sin(), xfreq.cos()], dim=1)\neps = 0.000\ncoeffs = torch.linalg.solve(X.t() @ X + torch.eye(X.shape[1]) *eps, X.t() @ y)\nx2 = torch.arange(0, 2 *L)\nxfreq2 = torch.outer(x2, freqs)\nX2 = torch.cat([xfreq2.sin(), xfreq2.cos()], dim=1)\ny2 = X2 @ coeffs\nx3 = torch.arange(25, 75, 0.125)\nxfreq3 = torch.outer(x3, freqs)\nX3 = torch.cat([xfreq3.sin(), xfreq3.cos()], dim=1)\ny3 = X3 @ coeffs\nplt.figure(figsize=(16,5))\nplt.subplot(1, 3, 1)\nplt.plot(x2[:L], y2[:L], \"r\")\nplt.scatter(x, y)\nplt.ylabel(\"attention score $a(s)$\")\nplt.xlabel(\"Positional difference $s$\")\nplt.subplot(1, 3, 2)\nplt.plot(x2, y2, \"r\")\nplt.scatter(x, y)\nplt.axvline(L, color=\"k\", linestyle=\"--\", linewidth=0.5)\nplt.title(\"Effect of Extrapolation\")\nplt.xlabel(\"Positional difference $s$\")\nplt.subplot(1, 3, 3)\nplt.plot(x3, y3, \"r\")\nfor i in range(25,75):\nplt.axvline(i, color=\"k\", linestyle=\"--\", linewidth=0.5)\nplt.title(\"Effect of Interpolation\")\nplt.xlabel(\"Positional difference $s$\")\nplt.show()\n17\nC.2 C ODE FOR FIG. 5\nL = 2048\nx = torch.arange(0, 2 *L)\nd = 4096 // 32\ntheta = 10000\nfreqs = 1.0 / (theta **(torch.arange(0, d, 2)[: (d // 2)].float() / d))\nxfreq = torch.outer(x, freqs)\nmags = (xfreq.sin().cumsum(dim=1).pow(2) + xfreq.cos().cumsum(dim=1).pow(2)).sqrt()\nplt.plot(mags.sum(dim=1)/d)\nplt.axhline(1.0, color=’k’, linestyle=\"--\")\nplt.xlabel(\"Positional difference $s$\")\nplt.ylabel(\"$B(s)/d$\")\nplt.show()\n18"
        ],
        "subcomponents": {},
        "metadata": {
          "path": "value",
          "lambda": "https://qlroxye6wq3y47l5bmsl4akfba0jzemj.lambda-url.us-east-1.on.aws/",
          "args": [
            {
              "name": "file",
              "ty": {},
              "type": "File",
              "default": null
            }
          ],
          "name": "Pdf2Txt",
          "operator": "pdf2txt",
          "description": "Extracts text from PDF",
          "num_outputs": 1
        },
        "extension": {
          "name": "pdfutils2",
          "prod": true,
          "type": "Lambda",
          "tag": "0.0.1",
          "id": 88,
          "store": true
        },
        "remote": {
          "module": "Base Node",
          "scope": "pdfutils",
          "url": "https://static.236409319020.oloren.aws.olorencore.com/web/store/lambda/pdfutils2/0.0.1/ui/remoteEntry.js"
        },
        "retries": 2,
        "input_handles": {
          "0": 80
        },
        "lastNodeId": 76332,
        "log_example": {
          "0": "content"
        }
      },
      "position": {
        "x": 633,
        "y": 165
      },
      "type": "RemoteNode",
      "width": 402,
      "height": 158,
      "selected": false,
      "positionAbsolute": {
        "x": 633,
        "y": 165
      },
      "dragging": false
    },
    {
      "id": "node-2",
      "data": {
        "data": {},
        "num_inputs": 1,
        "num_outputs": 0,
        "operator": "ui",
        "hierarchy": [],
        "logs": "",
        "status": "finished",
        "outputs": [],
        "subcomponents": {},
        "metadata": {
          "name": "Display JSON",
          "path": "./src/nodes/display/json/Base.tsx",
          "applet": {
            "path": "./src/nodes/display/json/Applet.tsx"
          },
          "num_inputs": 1,
          "num_outputs": 0
        },
        "extension": {
          "name": "basics",
          "prod": true,
          "type": "React",
          "tag": "latest",
          "id": 20,
          "store": false
        },
        "remote": {
          "module": "Display JSON",
          "scope": "testing",
          "url": "https://static.236409319020.oloren.aws.olorencore.com/web/react/basicsaf4f82cb-18a0-4995-b375-76955354c302/remoteEntry.js"
        },
        "retries": 2,
        "lastNodeId": 76338
      },
      "position": {
        "x": 1174.867677486004,
        "y": 188.26046878667125
      },
      "type": "RemoteNode",
      "width": 254,
      "height": 96,
      "selected": false,
      "positionAbsolute": {
        "x": 1174.867677486004,
        "y": 188.26046878667125
      },
      "dragging": false
    }
  ],
  "edges": [
    {
      "source": "node-0",
      "sourceHandle": "0",
      "target": "node-1",
      "targetHandle": "0",
      "type": "FlowEdge",
      "id": "node-0.0#node-1.0",
      "data": {
        "pmode": "Input",
        "ptype": "Json",
        "pname": "pdf"
      },
      "markerEnd": {
        "type": "arrowclosed",
        "width": 6,
        "height": 6,
        "color": "#000"
      },
      "selected": false
    },
    {
      "source": "node-1",
      "sourceHandle": "0",
      "target": "node-2",
      "targetHandle": "0",
      "type": "FlowEdge",
      "id": "node-1.0#node-2.0",
      "data": {
        "pmode": "Output",
        "ptype": "Json",
        "pname": "content"
      },
      "markerEnd": {
        "type": "arrowclosed",
        "width": 6,
        "height": 6,
        "color": "#000"
      },
      "selected": false
    }
  ],
  "exportName": "pdf2txt"
}